{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Machine learning uses algorithms that can figure out how to perform important tasks by generalizing from examples\n",
    "\n",
    "Supervised learning involves inferring a function from labeled training data to make predictions on unseen data\n",
    "\n",
    "Unsupervised learning involves deriving structure from data where we don't know the effect of any of the variables\n",
    "\n",
    "The spam filter classification problem we are dealing with involves supervised learning\n",
    "\n",
    "# K-fold Cross Validation\n",
    "\n",
    "In k-folds cross validation, the full dataset is divided into k subsets and the holdout method is repeated k times\n",
    "\n",
    "Each time, one of the k subsets is used as the test set and the other k-1 subsets are used to train the model\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "Accuracy = # predicted correctly / total\n",
    "\n",
    "Precision = # correctly predicted as spam / total predicted spam\n",
    "\n",
    "Recall = # correctly predicted as spam / total spam\n",
    "\n",
    "If false positives are very costly, we will want to optimize for precision\n",
    "\n",
    "If false negatives are very costly, we will want to optimize for recall\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "Random forest is an ensemble model, meaning it creates many models then combines them to create a mega model\n",
    "\n",
    "The idea is to create a bunch of relatively weak models that can combine to make a strong model (each model votes on a prediction value)\n",
    "\n",
    "Random forest models construct a collection of decision trees then aggregate the predictions of each tree to determine the final prediction\n",
    "\n",
    "There are many benefits to random forests and ensemble methods:\n",
    "\n",
    "-- Compatible with classification and regression problems\n",
    "\n",
    "-- Can easily handle outliers, missing data, etc.\n",
    "\n",
    "-- Accepts various types of data (ordinal, continuous, etc.)\n",
    "\n",
    "-- Less likely to overfit\n",
    "\n",
    "-- Outputs feature importance\n",
    "\n",
    "We will build a random forest model in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "\n",
    "stops = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer\n",
    "\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t', header = None)\n",
    "\n",
    "data.columns = ['label', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new features - need to make count punctuation function first\n",
    "\n",
    "def count_punc(t):\n",
    "    \n",
    "    num_puncs = sum([1 for x in t if x in string.punctuation])\n",
    "    \n",
    "    return round(num_puncs / (len(t) - t.count(\" \")), 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new features\n",
    "\n",
    "data['text_length'] = data['text'].apply(lambda x: len(x))\n",
    "\n",
    "data['percent_punc'] = data['text'].apply(lambda x: count_punc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>percent_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>196</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>77</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>35</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  text_length  \\\n",
       "0   ham  I've been searching for the right words to tha...          196   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...          155   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...           61   \n",
       "3   ham  Even my brother is not like to speak with me. ...           77   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!           35   \n",
       "\n",
       "   percent_punc  \n",
       "0           2.5  \n",
       "1           4.7  \n",
       "2           4.1  \n",
       "3           3.2  \n",
       "4           7.1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clean text function\n",
    "\n",
    "def clean_text(t):\n",
    "    \n",
    "    text = \"\".join([x.lower() for x in t if x not in string.punctuation])\n",
    "    \n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "    text = [ps.stem(x) for x in tokens if x not in stops]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TFIDF vectorizer with clean text analyzer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(analyzer=clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform vectorizer\n",
    "\n",
    "X_tfidf = tfidf_vec.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another object for feature only (no labels)\n",
    "\n",
    "X_features = pd.concat([data['text_length'], data['percent_punc'], pd.DataFrame(X_tfidf.toarray())], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_length</th>\n",
       "      <th>percent_punc</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8097</th>\n",
       "      <th>8098</th>\n",
       "      <th>8099</th>\n",
       "      <th>8100</th>\n",
       "      <th>8101</th>\n",
       "      <th>8102</th>\n",
       "      <th>8103</th>\n",
       "      <th>8104</th>\n",
       "      <th>8105</th>\n",
       "      <th>8106</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_length  percent_punc    0    1    2    3    4    5    6    7  ...   \\\n",
       "0          196           2.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    \n",
       "1          155           4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    \n",
       "2           61           4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    \n",
       "3           77           3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    \n",
       "4           35           7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    \n",
       "\n",
       "   8097  8098  8099  8100  8101  8102  8103  8104  8105  8106  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8109 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out feature data\n",
    "\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random forest classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " '_make_estimator',\n",
       " '_set_oob_score',\n",
       " '_validate_X_predict',\n",
       " '_validate_estimator',\n",
       " '_validate_y_class_weight',\n",
       " 'apply',\n",
       " 'decision_path',\n",
       " 'feature_importances_',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'score',\n",
       " 'set_params']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what attributes and methods are contained in this object?\n",
    "\n",
    "dir(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# note that feature importances will give us info on the most helpful features for the model\n",
    "\n",
    "# fit will allow us to fit the model, while predict will allow us to make predictions on new test data\n",
    "\n",
    "print(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max depth is how deep our decision tree is - default is no max depth\n",
    "\n",
    "# n estimators is how many trees will be built in the random forest - default is 10\n",
    "\n",
    "# we will want to run the model through cross validation - need to import\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kfolds will split our data into cross validation subsets\n",
    "\n",
    "# cross val score will tell us how our model scored on each subset\n",
    "\n",
    "# start by creating instance of RandomForestClassifier() - n_jobs=-1 allows us to run the model faster in parallel\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create KFolds object with 5 cross validation subsets\n",
    "\n",
    "k_fold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96947935, 0.96947935, 0.96409336, 0.9640611 , 0.96855346])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see the cross val score object using random forest model as estimator (first argument)\n",
    "\n",
    "# we then pass in the features (X values) and the spam/ham labels (y values) and the kfolds object as the cv argument\n",
    "\n",
    "# we will start by using accuracy as our scoring method and set n_jobs = -1 to run parallel calculations\n",
    "\n",
    "cross_val_score(rf, X_features, data['label'], cv = k_fold, scoring = 'accuracy', n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now create and use a holdout test set to evaluation our model performance\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training and testing data with 20% of data in test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new random forest classifier with 50 trees and max depth of 20\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 50, max_depth = 20, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the random forest classifier into a model\n",
    "\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.07820339173773969, 'text_length'),\n",
       " (0.04074969502520828, 7353),\n",
       " (0.03632211190646081, 1804),\n",
       " (0.030691484822236527, 5727),\n",
       " (0.028108417496141336, 2032),\n",
       " (0.021775227340367763, 7030),\n",
       " (0.021743903809094082, 3135),\n",
       " (0.01989730051490501, 6749),\n",
       " (0.018103013080741356, 6288),\n",
       " (0.014577571923090884, 690)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check out the feature importances of the model\n",
    "\n",
    "sorted(zip(rf_model.feature_importances_, X_train.columns), reverse = True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets predict values on the testing data and assign to an object\n",
    "\n",
    "pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score our predictions - the pos label is the label we are interested in predicting (in this case, spam)\n",
    "\n",
    "# we need to assign to 4 diff values for 4 diff outputs from score function\n",
    "\n",
    "precision, recall, fscore, support = score(y_test, pred, pos_label = 'spam', average = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 \n",
      "Recall: 0.582 \n",
      "Accuracy: 0.945\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {round(precision,3)} \\nRecall: {round(recall,3)} \\nAccuracy: {round((pred==y_test).sum()/len(y_test),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we make our model better by changing the hyperparameter settings?\n",
    "\n",
    "# We can find out using a Grid Search\n",
    "\n",
    "# We will approach this by defining a function with entire RF training and prediction process\n",
    "\n",
    "def train_rf(n_est, depth):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs = -1)\n",
    "    \n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    \n",
    "    pred = rf_model.predict(X_test)\n",
    "    \n",
    "    precision, recall, fscore, support = score(y_test, pred, pos_label='spam', average='binary')\n",
    "    \n",
    "    print(f'Est: {n_est}, Depth: {depth}')\n",
    "    print(f'Precision: {round(precision,3)}, Recall: {round(recall,3)}, Accuracy: {round((pred==y_test).sum()/len(y_test),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 10, Depth: 10\n",
      "Precision: 1.0, Recall: 0.219, Accuracy: 0.898\n",
      "Est: 10, Depth: 20\n",
      "Precision: 1.0, Recall: 0.521, Accuracy: 0.937\n",
      "Est: 10, Depth: 30\n",
      "Precision: 1.0, Recall: 0.637, Accuracy: 0.952\n",
      "Est: 10, Depth: None\n",
      "Precision: 1.0, Recall: 0.76, Accuracy: 0.969\n",
      "Est: 50, Depth: 10\n",
      "Precision: 1.0, Recall: 0.253, Accuracy: 0.902\n",
      "Est: 50, Depth: 20\n",
      "Precision: 1.0, Recall: 0.5, Accuracy: 0.934\n",
      "Est: 50, Depth: 30\n",
      "Precision: 1.0, Recall: 0.692, Accuracy: 0.96\n",
      "Est: 50, Depth: None\n",
      "Precision: 1.0, Recall: 0.788, Accuracy: 0.972\n",
      "Est: 100, Depth: 10\n",
      "Precision: 1.0, Recall: 0.219, Accuracy: 0.898\n",
      "Est: 100, Depth: 20\n",
      "Precision: 1.0, Recall: 0.562, Accuracy: 0.943\n",
      "Est: 100, Depth: 30\n",
      "Precision: 1.0, Recall: 0.664, Accuracy: 0.956\n",
      "Est: 100, Depth: None\n",
      "Precision: 1.0, Recall: 0.815, Accuracy: 0.976\n"
     ]
    }
   ],
   "source": [
    "for n_est in [10, 50, 100]:\n",
    "        \n",
    "        for depth in [10, 20, 30, None]:\n",
    "            \n",
    "            train_rf(n_est, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now start using GridSearchCV to assist us in our parameter tuning\n",
    "\n",
    "# start by creating a count vectorized document term matrix to compare against TFIDF matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer(analyzer = clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count = count_vec.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new feature sets\n",
    "\n",
    "X_tfidf_feat = pd.concat([data['text_length'], data['percent_punc'], pd.DataFrame(X_tfidf.toarray())], axis = 1)\n",
    "\n",
    "X_count_feat = pd.concat([data['text_length'], data['percent_punc'], pd.DataFrame(X_count.toarray())], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in GridSearchCV and Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our parameter grid will be a dictionary called param\n",
    "\n",
    "# Key values will be parameter names and values will be ranges to explore\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "param = {'n_estimators':range(50, 301, 50), 'max_depth':range(30, 151, 30)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can run GridSearchCV and assign to gs\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can fit gs like any other model - run for both TFIDF and Count Matrices\n",
    "\n",
    "gs_tfidf = gs.fit(X_tfidf_feat, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.461405</td>\n",
       "      <td>2.467248</td>\n",
       "      <td>0.523001</td>\n",
       "      <td>0.177135</td>\n",
       "      <td>120</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 120, 'n_estimators': 50}</td>\n",
       "      <td>0.979372</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975395</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999776</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999820</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16.171962</td>\n",
       "      <td>1.387190</td>\n",
       "      <td>0.451994</td>\n",
       "      <td>0.151383</td>\n",
       "      <td>90</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 50}</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.978456</td>\n",
       "      <td>0.976640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975216</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.998204</td>\n",
       "      <td>0.998967</td>\n",
       "      <td>0.000462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>29.632629</td>\n",
       "      <td>1.124234</td>\n",
       "      <td>0.545938</td>\n",
       "      <td>0.202112</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 100}</td>\n",
       "      <td>0.981166</td>\n",
       "      <td>0.979354</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975216</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999237</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>50.632745</td>\n",
       "      <td>3.199368</td>\n",
       "      <td>0.552323</td>\n",
       "      <td>0.066546</td>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 150, 'n_estimators': 200}</td>\n",
       "      <td>0.981166</td>\n",
       "      <td>0.980251</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975036</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>54.589573</td>\n",
       "      <td>6.306942</td>\n",
       "      <td>0.333509</td>\n",
       "      <td>0.091739</td>\n",
       "      <td>150</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 150, 'n_estimators': 300}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.979354</td>\n",
       "      <td>0.973046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974677</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "18      18.461405      2.467248         0.523001        0.177135   \n",
       "12      16.171962      1.387190         0.451994        0.151383   \n",
       "13      29.632629      1.124234         0.545938        0.202112   \n",
       "27      50.632745      3.199368         0.552323        0.066546   \n",
       "29      54.589573      6.306942         0.333509        0.091739   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "18             120                 50   \n",
       "12              90                 50   \n",
       "13              90                100   \n",
       "27             150                200   \n",
       "29             150                300   \n",
       "\n",
       "                                     params  split0_test_score  \\\n",
       "18   {'max_depth': 120, 'n_estimators': 50}           0.979372   \n",
       "12    {'max_depth': 90, 'n_estimators': 50}           0.975785   \n",
       "13   {'max_depth': 90, 'n_estimators': 100}           0.981166   \n",
       "27  {'max_depth': 150, 'n_estimators': 200}           0.981166   \n",
       "29  {'max_depth': 150, 'n_estimators': 300}           0.978475   \n",
       "\n",
       "    split1_test_score  split2_test_score       ...         mean_test_score  \\\n",
       "18           0.976661           0.975741       ...                0.975395   \n",
       "12           0.978456           0.976640       ...                0.975216   \n",
       "13           0.979354           0.974843       ...                0.975216   \n",
       "27           0.980251           0.974843       ...                0.975036   \n",
       "29           0.979354           0.973046       ...                0.974677   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "18        0.002593                1            1.000000            0.999775   \n",
       "12        0.002458                2            0.998877            0.998877   \n",
       "13        0.004787                2            0.999326            0.999102   \n",
       "27        0.005076                4            1.000000            1.000000   \n",
       "29        0.003535                5            1.000000            1.000000   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "18            1.000000            0.999776            0.999551   \n",
       "12            0.999327            0.999551            0.998204   \n",
       "13            0.999102            0.999327            0.999327   \n",
       "27            1.000000            1.000000            1.000000   \n",
       "29            1.000000            1.000000            1.000000   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "18          0.999820         0.000168  \n",
       "12          0.998967         0.000462  \n",
       "13          0.999237         0.000110  \n",
       "27          1.000000         0.000000  \n",
       "29          1.000000         0.000000  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see the results from each fold in this model - we clean it up in a dataframe and sort by avg test score\n",
    "\n",
    "# we will use head to only look at top 5 models\n",
    "\n",
    "pd.DataFrame(gs_tfidf.cv_results_).sort_values('mean_test_score', ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it all again with the count matrix instead of the tfidf matrix\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "param = {'n_estimators':range(50, 301, 50), 'max_depth':range(30, 151, 30)}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv = 5, n_jobs = -1)\n",
    "\n",
    "gs_count = gs.fit(X_count_feat, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>70.846042</td>\n",
       "      <td>2.200965</td>\n",
       "      <td>0.770990</td>\n",
       "      <td>0.084079</td>\n",
       "      <td>150</td>\n",
       "      <td>250</td>\n",
       "      <td>{'max_depth': 150, 'n_estimators': 250}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.975763</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973779</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>69.897827</td>\n",
       "      <td>0.689851</td>\n",
       "      <td>0.858779</td>\n",
       "      <td>0.088106</td>\n",
       "      <td>120</td>\n",
       "      <td>250</td>\n",
       "      <td>{'max_depth': 120, 'n_estimators': 250}</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973420</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>60.197678</td>\n",
       "      <td>3.252490</td>\n",
       "      <td>0.632771</td>\n",
       "      <td>0.049472</td>\n",
       "      <td>120</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 120, 'n_estimators': 200}</td>\n",
       "      <td>0.980269</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973240</td>\n",
       "      <td>0.004089</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>49.499514</td>\n",
       "      <td>1.229818</td>\n",
       "      <td>0.706652</td>\n",
       "      <td>0.128027</td>\n",
       "      <td>120</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 120, 'n_estimators': 150}</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.972172</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973240</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31.003665</td>\n",
       "      <td>0.295832</td>\n",
       "      <td>0.734116</td>\n",
       "      <td>0.255641</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 120, 'n_estimators': 100}</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.973968</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973240</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999776</td>\n",
       "      <td>0.999820</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "28      70.846042      2.200965         0.770990        0.084079   \n",
       "22      69.897827      0.689851         0.858779        0.088106   \n",
       "21      60.197678      3.252490         0.632771        0.049472   \n",
       "20      49.499514      1.229818         0.706652        0.128027   \n",
       "19      31.003665      0.295832         0.734116        0.255641   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "28             150                250   \n",
       "22             120                250   \n",
       "21             120                200   \n",
       "20             120                150   \n",
       "19             120                100   \n",
       "\n",
       "                                     params  split0_test_score  \\\n",
       "28  {'max_depth': 150, 'n_estimators': 250}           0.978475   \n",
       "22  {'max_depth': 120, 'n_estimators': 250}           0.977578   \n",
       "21  {'max_depth': 120, 'n_estimators': 200}           0.980269   \n",
       "20  {'max_depth': 120, 'n_estimators': 150}           0.977578   \n",
       "19  {'max_depth': 120, 'n_estimators': 100}           0.976682   \n",
       "\n",
       "    split1_test_score  split2_test_score       ...         mean_test_score  \\\n",
       "28           0.975763           0.974843       ...                0.973779   \n",
       "22           0.976661           0.974843       ...                0.973420   \n",
       "21           0.974865           0.972147       ...                0.973240   \n",
       "20           0.972172           0.974843       ...                0.973240   \n",
       "19           0.973968           0.973944       ...                0.973240   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "28        0.003488                1            1.000000            1.000000   \n",
       "22        0.003722                2            1.000000            1.000000   \n",
       "21        0.004089                3            0.999775            1.000000   \n",
       "20        0.002760                3            0.999775            1.000000   \n",
       "19        0.002245                3            1.000000            0.999775   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "28            1.000000                 1.0            1.000000   \n",
       "22            1.000000                 1.0            1.000000   \n",
       "21            1.000000                 1.0            1.000000   \n",
       "20            1.000000                 1.0            1.000000   \n",
       "19            0.999551                 1.0            0.999776   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "28          1.000000         0.000000  \n",
       "22          1.000000         0.000000  \n",
       "21          0.999955         0.000090  \n",
       "20          0.999955         0.000090  \n",
       "19          0.999820         0.000168  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at results from count vectorizer model\n",
    "\n",
    "pd.DataFrame(gs_count.cv_results_).sort_values('mean_test_score', ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFIDF vectorizer yielded the best predictions with a smaller number of estimators\n",
    "\n",
    "The count vectorizer yielded the best predictions with more estimators\n",
    "\n",
    "For both, the most accurate decision trees are those with the largest max_depth\n",
    "\n",
    "However, note that mean fit time is much lower when depth and number of estimators are lower\n",
    "\n",
    "Note that we would normally explore a lot more options:\n",
    "\n",
    "-- Check whether or not including stop words is helpful\n",
    "\n",
    "-- Check whether removing punctuation is helpful\n",
    "\n",
    "-- Check results with n-grams\n",
    "\n",
    "# Gradient Boosting Models\n",
    "\n",
    "Gradient Boosting is another ensemble model similar to random forests (many models built and combined to make one powerful model)\n",
    "\n",
    "It takes an iterative approach to combining weak learners to create a strong learner by focusing on mistakes of prior iterations\n",
    "\n",
    "The first trees are tiny (essentially stumps), but the model continues to focus on what it got wrong in the previous model\n",
    "\n",
    "The main difference between Gradient Boosting is it uses Boosting (increased weight on wrong predictions) while Random Forest uses Bagging (random sampling)\n",
    "\n",
    "Unlike random forest, a gradient boosting model cannot train trees in parallel - it must be done iteratively\n",
    "\n",
    "Gradient boosting uses a weighted vote for the final prediction, while random forest uses an unweighted prediction\n",
    "\n",
    "Gradient boosting models are also harder to tune and easier to overfit than random forests\n",
    "\n",
    "The trade off is that gradient boosting models are typically more powerful if tuned properly\n",
    "\n",
    "Time to code it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to train gradient boosting model - we add another argument for learning rate parameter\n",
    "\n",
    "def train_gb(est, max_depth, lr):\n",
    "    \n",
    "    gb = GradientBoostingClassifier(n_estimators=est, max_depth=max_depth, learning_rate=lr)\n",
    "    \n",
    "    gb_model = gb.fit(X_train, y_train)\n",
    "    \n",
    "    pred = gb_model.predict(X_test)\n",
    "    \n",
    "    precision, recall, fscore, support = score(y_test, pred, pos_label='spam', average='binary')\n",
    "    \n",
    "    print(f\"Est: {est}, Max Depth: {max_depth}, Learning Rate: {lr}\")\n",
    "    print(f\"Precision: {round(precision,3)}\") \n",
    "    print(f\"Recall: {round(recall,3)}\") \n",
    "    print(f\"Accuracy: {round((y_test==pred).sum()/len(pred),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 50, Max Depth: 3, Learning Rate: 0.01\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Accuracy: 0.869\n",
      "Est: 50, Max Depth: 3, Learning Rate: 0.1\n",
      "Precision: 0.945\n",
      "Recall: 0.705\n",
      "Accuracy: 0.956\n",
      "Est: 50, Max Depth: 3, Learning Rate: 1\n",
      "Precision: 0.862\n",
      "Recall: 0.767\n",
      "Accuracy: 0.953\n",
      "Est: 50, Max Depth: 7, Learning Rate: 0.01\n",
      "Precision: 1.0\n",
      "Recall: 0.007\n",
      "Accuracy: 0.87\n",
      "Est: 50, Max Depth: 7, Learning Rate: 0.1\n",
      "Precision: 0.938\n",
      "Recall: 0.822\n",
      "Accuracy: 0.969\n",
      "Est: 50, Max Depth: 7, Learning Rate: 1\n",
      "Precision: 0.889\n",
      "Recall: 0.822\n",
      "Accuracy: 0.963\n",
      "Est: 50, Max Depth: 11, Learning Rate: 0.01\n",
      "Precision: 1.0\n",
      "Recall: 0.021\n",
      "Accuracy: 0.872\n",
      "Est: 50, Max Depth: 11, Learning Rate: 0.1\n",
      "Precision: 0.924\n",
      "Recall: 0.829\n",
      "Accuracy: 0.969\n",
      "Est: 50, Max Depth: 11, Learning Rate: 1\n",
      "Precision: 0.912\n",
      "Recall: 0.856\n",
      "Accuracy: 0.97\n",
      "Est: 50, Max Depth: 15, Learning Rate: 0.01\n",
      "Precision: 1.0\n",
      "Recall: 0.014\n",
      "Accuracy: 0.871\n",
      "Est: 50, Max Depth: 15, Learning Rate: 0.1\n",
      "Precision: 0.952\n",
      "Recall: 0.822\n",
      "Accuracy: 0.971\n",
      "Est: 50, Max Depth: 15, Learning Rate: 1\n",
      "Precision: 0.897\n",
      "Recall: 0.836\n",
      "Accuracy: 0.966\n",
      "Est: 100, Max Depth: 3, Learning Rate: 0.01\n",
      "Precision: 0.961\n",
      "Recall: 0.5\n",
      "Accuracy: 0.932\n",
      "Est: 100, Max Depth: 3, Learning Rate: 0.1\n",
      "Precision: 0.95\n",
      "Recall: 0.788\n",
      "Accuracy: 0.967\n",
      "Est: 100, Max Depth: 3, Learning Rate: 1\n",
      "Precision: 0.88\n",
      "Recall: 0.753\n",
      "Accuracy: 0.954\n",
      "Est: 100, Max Depth: 7, Learning Rate: 0.01\n",
      "Precision: 0.951\n",
      "Recall: 0.671\n",
      "Accuracy: 0.952\n",
      "Est: 100, Max Depth: 7, Learning Rate: 0.1\n",
      "Precision: 0.946\n",
      "Recall: 0.836\n",
      "Accuracy: 0.972\n",
      "Est: 100, Max Depth: 7, Learning Rate: 1\n",
      "Precision: 0.889\n",
      "Recall: 0.822\n",
      "Accuracy: 0.963\n",
      "Est: 100, Max Depth: 11, Learning Rate: 0.01\n",
      "Precision: 0.936\n",
      "Recall: 0.699\n",
      "Accuracy: 0.954\n",
      "Est: 100, Max Depth: 11, Learning Rate: 0.1\n",
      "Precision: 0.932\n",
      "Recall: 0.842\n",
      "Accuracy: 0.971\n",
      "Est: 100, Max Depth: 11, Learning Rate: 1\n",
      "Precision: 0.905\n",
      "Recall: 0.849\n",
      "Accuracy: 0.969\n",
      "Est: 100, Max Depth: 15, Learning Rate: 0.01\n",
      "Precision: 0.947\n",
      "Recall: 0.733\n",
      "Accuracy: 0.96\n",
      "Est: 100, Max Depth: 15, Learning Rate: 0.1\n",
      "Precision: 0.945\n",
      "Recall: 0.829\n",
      "Accuracy: 0.971\n",
      "Est: 100, Max Depth: 15, Learning Rate: 1\n",
      "Precision: 0.917\n",
      "Recall: 0.836\n",
      "Accuracy: 0.969\n",
      "Est: 150, Max Depth: 3, Learning Rate: 0.01\n",
      "Precision: 0.95\n",
      "Recall: 0.521\n",
      "Accuracy: 0.934\n",
      "Est: 150, Max Depth: 3, Learning Rate: 0.1\n",
      "Precision: 0.952\n",
      "Recall: 0.808\n",
      "Accuracy: 0.969\n",
      "Est: 150, Max Depth: 3, Learning Rate: 1\n",
      "Precision: 0.913\n",
      "Recall: 0.795\n",
      "Accuracy: 0.963\n",
      "Est: 150, Max Depth: 7, Learning Rate: 0.01\n",
      "Precision: 0.947\n",
      "Recall: 0.733\n",
      "Accuracy: 0.96\n",
      "Est: 150, Max Depth: 7, Learning Rate: 0.1\n",
      "Precision: 0.961\n",
      "Recall: 0.836\n",
      "Accuracy: 0.974\n",
      "Est: 150, Max Depth: 7, Learning Rate: 1\n",
      "Precision: 0.876\n",
      "Recall: 0.822\n",
      "Accuracy: 0.961\n",
      "Est: 150, Max Depth: 11, Learning Rate: 0.01\n",
      "Precision: 0.931\n",
      "Recall: 0.74\n",
      "Accuracy: 0.959\n",
      "Est: 150, Max Depth: 11, Learning Rate: 0.1\n",
      "Precision: 0.932\n",
      "Recall: 0.842\n",
      "Accuracy: 0.971\n",
      "Est: 150, Max Depth: 11, Learning Rate: 1\n",
      "Precision: 0.919\n",
      "Recall: 0.849\n",
      "Accuracy: 0.97\n",
      "Est: 150, Max Depth: 15, Learning Rate: 0.01\n",
      "Precision: 0.949\n",
      "Recall: 0.76\n",
      "Accuracy: 0.963\n",
      "Est: 150, Max Depth: 15, Learning Rate: 0.1\n",
      "Precision: 0.931\n",
      "Recall: 0.836\n",
      "Accuracy: 0.97\n",
      "Est: 150, Max Depth: 15, Learning Rate: 1\n",
      "Precision: 0.904\n",
      "Recall: 0.836\n",
      "Accuracy: 0.967\n"
     ]
    }
   ],
   "source": [
    "# time to tune model - this will take a while\n",
    "\n",
    "for n_est in [50, 100, 150]:\n",
    "    \n",
    "    for max_depth in [3, 7, 11, 15]:\n",
    "        \n",
    "        for lr in [0.01, 0.1, 1]:\n",
    "            \n",
    "            train_gb(n_est, max_depth, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the worst models had learning rates of 0.01 and lower values for max depth and number of estimators \n",
    "\n",
    "# the best performing models had a learning rate of 0.1 and higher values for max depth and number of estimators\n",
    "\n",
    "# moving onto next phase with GridSearchCV for gradient boosting\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate gb classifier and create dictionary of parameters\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "param = {\n",
    "    'n_estimators':[100, 150],\n",
    "    'max_depth':[12, 15],\n",
    "    'learning_rate':[0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GridSearchCV model - note that n_jobs = -1 does not train models in parallel, only parameter settings\n",
    "\n",
    "gs = GridSearchCV(gb, param, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model with tfidf - this could take a while\n",
    "\n",
    "gs_tfidf = gs.fit(X_tfidf_feat, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>411.058594</td>\n",
       "      <td>22.773779</td>\n",
       "      <td>0.435553</td>\n",
       "      <td>0.043537</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.2, 'max_depth': 12, 'n_est...</td>\n",
       "      <td>0.963229</td>\n",
       "      <td>0.980251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970726</td>\n",
       "      <td>0.005633</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>547.091751</td>\n",
       "      <td>8.083826</td>\n",
       "      <td>0.776125</td>\n",
       "      <td>0.310329</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.2, 'max_depth': 12, 'n_est...</td>\n",
       "      <td>0.965022</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970546</td>\n",
       "      <td>0.004220</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>541.780269</td>\n",
       "      <td>8.193791</td>\n",
       "      <td>0.252728</td>\n",
       "      <td>0.050285</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.2, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.963229</td>\n",
       "      <td>0.981149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970187</td>\n",
       "      <td>0.006044</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>464.675907</td>\n",
       "      <td>13.922152</td>\n",
       "      <td>0.627123</td>\n",
       "      <td>0.135780</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.2, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.961435</td>\n",
       "      <td>0.981149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968930</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>451.457653</td>\n",
       "      <td>6.657469</td>\n",
       "      <td>0.483105</td>\n",
       "      <td>0.048404</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.961435</td>\n",
       "      <td>0.979354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968570</td>\n",
       "      <td>0.006032</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "4     411.058594     22.773779         0.435553        0.043537   \n",
       "5     547.091751      8.083826         0.776125        0.310329   \n",
       "7     541.780269      8.193791         0.252728        0.050285   \n",
       "6     464.675907     13.922152         0.627123        0.135780   \n",
       "2     451.457653      6.657469         0.483105        0.048404   \n",
       "\n",
       "  param_learning_rate param_max_depth param_n_estimators  \\\n",
       "4                 0.2              12                100   \n",
       "5                 0.2              12                150   \n",
       "7                 0.2              15                150   \n",
       "6                 0.2              15                100   \n",
       "2                 0.1              15                100   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "4  {'learning_rate': 0.2, 'max_depth': 12, 'n_est...           0.963229   \n",
       "5  {'learning_rate': 0.2, 'max_depth': 12, 'n_est...           0.965022   \n",
       "7  {'learning_rate': 0.2, 'max_depth': 15, 'n_est...           0.963229   \n",
       "6  {'learning_rate': 0.2, 'max_depth': 15, 'n_est...           0.961435   \n",
       "2  {'learning_rate': 0.1, 'max_depth': 15, 'n_est...           0.961435   \n",
       "\n",
       "   split1_test_score       ...         mean_test_score  std_test_score  \\\n",
       "4           0.980251       ...                0.970726        0.005633   \n",
       "5           0.976661       ...                0.970546        0.004220   \n",
       "7           0.981149       ...                0.970187        0.006044   \n",
       "6           0.981149       ...                0.968930        0.006682   \n",
       "2           0.979354       ...                0.968570        0.006032   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "4                1                 1.0                 1.0   \n",
       "5                2                 1.0                 1.0   \n",
       "7                3                 1.0                 1.0   \n",
       "6                4                 1.0                 1.0   \n",
       "2                5                 1.0                 1.0   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "4                 1.0                 1.0                 1.0   \n",
       "5                 1.0                 1.0                 1.0   \n",
       "7                 1.0                 1.0                 1.0   \n",
       "6                 1.0                 1.0                 1.0   \n",
       "2                 1.0                 1.0                 1.0   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "4               1.0              0.0  \n",
       "5               1.0              0.0  \n",
       "7               1.0              0.0  \n",
       "6               1.0              0.0  \n",
       "2               1.0              0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create DataFrame for results, sort by mean test score, then show top 5 \n",
    "\n",
    "pd.DataFrame(gs_tfidf.cv_results_).sort_values('mean_test_score', ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\casey\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>702.508892</td>\n",
       "      <td>7.215908</td>\n",
       "      <td>0.384342</td>\n",
       "      <td>0.057611</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.2, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.965919</td>\n",
       "      <td>0.982047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972522</td>\n",
       "      <td>0.005606</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>513.285822</td>\n",
       "      <td>3.617929</td>\n",
       "      <td>0.456880</td>\n",
       "      <td>0.080212</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.2, 'max_depth': 12, 'n_est...</td>\n",
       "      <td>0.968610</td>\n",
       "      <td>0.982944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972342</td>\n",
       "      <td>0.005402</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>461.243162</td>\n",
       "      <td>17.317166</td>\n",
       "      <td>0.567244</td>\n",
       "      <td>0.188291</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.2, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.965022</td>\n",
       "      <td>0.981149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971803</td>\n",
       "      <td>0.005370</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>553.485005</td>\n",
       "      <td>5.391244</td>\n",
       "      <td>0.582840</td>\n",
       "      <td>0.109097</td>\n",
       "      <td>0.1</td>\n",
       "      <td>12</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 12, 'n_est...</td>\n",
       "      <td>0.961435</td>\n",
       "      <td>0.979354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970187</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>335.422707</td>\n",
       "      <td>3.050208</td>\n",
       "      <td>0.403925</td>\n",
       "      <td>0.073323</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.2, 'max_depth': 12, 'n_est...</td>\n",
       "      <td>0.968610</td>\n",
       "      <td>0.978456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970187</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "7     702.508892      7.215908         0.384342        0.057611   \n",
       "5     513.285822      3.617929         0.456880        0.080212   \n",
       "6     461.243162     17.317166         0.567244        0.188291   \n",
       "1     553.485005      5.391244         0.582840        0.109097   \n",
       "4     335.422707      3.050208         0.403925        0.073323   \n",
       "\n",
       "  param_learning_rate param_max_depth param_n_estimators  \\\n",
       "7                 0.2              15                150   \n",
       "5                 0.2              12                150   \n",
       "6                 0.2              15                100   \n",
       "1                 0.1              12                150   \n",
       "4                 0.2              12                100   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "7  {'learning_rate': 0.2, 'max_depth': 15, 'n_est...           0.965919   \n",
       "5  {'learning_rate': 0.2, 'max_depth': 12, 'n_est...           0.968610   \n",
       "6  {'learning_rate': 0.2, 'max_depth': 15, 'n_est...           0.965022   \n",
       "1  {'learning_rate': 0.1, 'max_depth': 12, 'n_est...           0.961435   \n",
       "4  {'learning_rate': 0.2, 'max_depth': 12, 'n_est...           0.968610   \n",
       "\n",
       "   split1_test_score       ...         mean_test_score  std_test_score  \\\n",
       "7           0.982047       ...                0.972522        0.005606   \n",
       "5           0.982944       ...                0.972342        0.005402   \n",
       "6           0.981149       ...                0.971803        0.005370   \n",
       "1           0.979354       ...                0.970187        0.005709   \n",
       "4           0.978456       ...                0.970187        0.004599   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "7                1                 1.0                 1.0   \n",
       "5                2                 1.0                 1.0   \n",
       "6                3                 1.0                 1.0   \n",
       "1                4                 1.0                 1.0   \n",
       "4                4                 1.0                 1.0   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "7                 1.0                 1.0                 1.0   \n",
       "5                 1.0                 1.0                 1.0   \n",
       "6                 1.0                 1.0                 1.0   \n",
       "1                 1.0                 1.0                 1.0   \n",
       "4                 1.0                 1.0                 1.0   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "7               1.0              0.0  \n",
       "5               1.0              0.0  \n",
       "6               1.0              0.0  \n",
       "1               1.0              0.0  \n",
       "4               1.0              0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same process for count vectorizer\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "param = {\n",
    "    'n_estimators':[100, 150],\n",
    "    'max_depth':[12, 15],\n",
    "    'learning_rate':[0.1, 0.2]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(gb, param, cv = 5, n_jobs = -1)\n",
    "\n",
    "gs_count = gs.fit(X_count_feat, data['label'])\n",
    "\n",
    "pd.DataFrame(gs_count.cv_results_).sort_values('mean_test_score', ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "To determine the best model, we need to use a training set and a test set\n",
    "\n",
    "This will tell us if our model is overfitting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time we will create train and test data from original data instead of vectorized data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['text', 'text_length', 'percent_punc']], \n",
    "                                                    data['label'],\n",
    "                                                    test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize with TFIDF \n",
    "\n",
    "tfidf_vec = TfidfVectorizer(analyzer=clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit training data into a tfidf matrix\n",
    "\n",
    "tfidf_vec_fit = tfidf_vec.fit(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to transform the train and test sets\n",
    "\n",
    "tfidf_vec_train = tfidf_vec.transform(X_train['text'])\n",
    "\n",
    "tfidf_vec_test = tfidf_vec.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the matrices with the other columns in the feature data to make new train and test dataframes\n",
    "\n",
    "X_train_vec = pd.concat([pd.DataFrame(tfidf_vec_train.toarray()), \n",
    "                        X_train[['text_length', 'percent_punc']].reset_index(drop=True)], \n",
    "                        axis=1)\n",
    "\n",
    "X_test_vec = pd.concat([pd.DataFrame(tfidf_vec_test.toarray()), \n",
    "                       X_test[['text_length', 'percent_punc']].reset_index(drop=True)], \n",
    "                       axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7129</th>\n",
       "      <th>7130</th>\n",
       "      <th>7131</th>\n",
       "      <th>7132</th>\n",
       "      <th>7133</th>\n",
       "      <th>7134</th>\n",
       "      <th>7135</th>\n",
       "      <th>7136</th>\n",
       "      <th>text_length</th>\n",
       "      <th>percent_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3    4    5    6    7    8    9      ...       7129  \\\n",
       "0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      ...        0.0   \n",
       "1  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      ...        0.0   \n",
       "2  0.287788  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      ...        0.0   \n",
       "3  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      ...        0.0   \n",
       "4  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      ...        0.0   \n",
       "\n",
       "   7130  7131  7132  7133  7134  7135  7136  text_length  percent_punc  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0           41           2.9  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0          142           6.8  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0           33          11.5  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0          152           0.8  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0           65           5.8  \n",
       "\n",
       "[5 rows x 7139 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see first 5 in both datasets\n",
    "\n",
    "X_train_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7129</th>\n",
       "      <th>7130</th>\n",
       "      <th>7131</th>\n",
       "      <th>7132</th>\n",
       "      <th>7133</th>\n",
       "      <th>7134</th>\n",
       "      <th>7135</th>\n",
       "      <th>7136</th>\n",
       "      <th>text_length</th>\n",
       "      <th>percent_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>126</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9      ...       7129  7130  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      ...        0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      ...        0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      ...        0.0   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      ...        0.0   0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      ...        0.0   0.0   \n",
       "\n",
       "   7131  7132  7133  7134  7135  7136  text_length  percent_punc  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0           96           2.6  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0           29           0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0           24          19.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0          165           1.4  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0          126           2.0  \n",
       "\n",
       "[5 rows x 7139 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that the data is ready, let's move onto the final model selection\n",
    "\n",
    "# make sure all packages are loaded\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build random forest model\n",
    "\n",
    "# we will also include extra lines of code to test how long it took to run the code\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "rf_model = rf.fit(X_train_vec, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "rf_fit_time = (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict classes and time code\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pred = rf_model.predict(X_test_vec)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "rf_pred_time = (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.839\n",
      "Accuracy: 0.978\n",
      "\n",
      "Time to fit: 5.696479797363281,\n",
      "Time to predict: 0.2303476333618164\n"
     ]
    }
   ],
   "source": [
    "# evaluate results\n",
    "\n",
    "precision, recall, fscore, support = score(y_test, pred, pos_label='spam', average='binary')\n",
    "\n",
    "print(f\"Precision: {round(precision, 3)}\\nRecall: {round(recall, 3)}\")\n",
    "\n",
    "print(f\"Accuracy: {round((pred==y_test).sum()/len(pred), 3)}\\n\")\n",
    "\n",
    "print(f\"Time to fit: {rf_fit_time},\\nTime to predict: {rf_pred_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build gradient boosting model\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "gb_model = gb.fit(X_train_vec, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "gb_fit_time = (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict classes and time code\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pred = gb_model.predict(X_test_vec)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "gb_pred_time = (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.918\n",
      "Recall: 0.826\n",
      "Accuracy: 0.967\n",
      "\n",
      "Time to fit: 400.3975188732147,\n",
      "Time to predict: 0.24857258796691895\n"
     ]
    }
   ],
   "source": [
    "# evaluate results\n",
    "\n",
    "precision, recall, fscore, support = score(y_test, pred, pos_label='spam', average='binary')\n",
    "\n",
    "print(f\"Precision: {round(precision, 3)}\\nRecall: {round(recall, 3)}\")\n",
    "\n",
    "print(f\"Accuracy: {round((pred==y_test).sum()/len(pred), 3)}\\n\")\n",
    "\n",
    "print(f\"Time to fit: {gb_fit_time},\\nTime to predict: {gb_pred_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two final points\n",
    "\n",
    "There would normally be a much more thorough evaluation of the model in a real-life scenario, including:\n",
    "\n",
    "-- Evaluation of performance on specific subsets (e.g. those with length 50 or more)\n",
    "\n",
    "-- Evaluation of specific messages the model is getting wrong\n",
    "\n",
    "The final model selection would be based on its alignment with the goals of the project (e.g. prioritizing precision or total accuracy)\n",
    "\n",
    "-- Is a longer predict time gonna make a big bottleneck in your process? Is it feasible to spend a long time training the model?\n",
    "\n",
    "-- Is there a higher price on precision (false positives) or recall (false negatives)\n",
    "\n",
    "-- In the spam filter we probably want to prioritize for precision, since we don't want people's real emails getting caught in the filter and the costs for a few spam messages are low\n",
    "\n",
    "-- In an antivirus software we would probably want to prioritize for recall, since there is a high price for falsely believing a software is not harmful when it actually is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('meaning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['some', 'of', 'the', 'words', 'are', 'combined']\n"
     ]
    }
   ],
   "source": [
    "print(re.split('\\W+',\"some of the-words are+combined\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'test', 'man', 'successful', 'lives']\n"
     ]
    }
   ],
   "source": [
    "s = \"This is a test for the man to be successful in their lives\"\n",
    "\n",
    "split_s = re.split('\\s+', s)\n",
    "\n",
    "slist = [x for x in split_s if x not in stops]\n",
    "\n",
    "print(slist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
